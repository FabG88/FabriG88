{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c551066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import h5py \n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import joblib\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1961a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the cancer dataset in a pandas dataframe.\n",
    "df_cancer = pd.read_csv(r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data cancer\\tc_labels.csv',header=None)\n",
    "#df.drop(2, inplace=True, axis=1)\n",
    "df_cancer.rename(columns = {0:'Folder name', 1:'label'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27bc3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the classes of our folders\n",
    "#Build the dataset\n",
    "df_til = pd.read_csv(r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data\\til_labels_binary.csv',header=None)\n",
    "#df_til.drop(2, inplace=True, axis=1)\n",
    "df_til.rename(columns = {0:'Folder name', 1:'label'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "275ee6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the folder TCGA-2Z-A9JL-01A-01-TS1 we have 1904 images.Label is: TUMOR\n",
      "In the folder TCGA-A3-3362-01A-01-BS1 we have 153 images.Label is: TUMOR\n",
      "In the folder TCGA-A3-3362-01A-02-BS2 we have 236 images.Label is: TUMOR\n",
      "In the folder TCGA-A3-A6NL-11A-01-TS1 we have 552 images.Label is: NORMAL\n",
      "In the folder TCGA-A3-A8CQ-01A-01-TS1 we have 944 images.Label is: TUMOR\n",
      "In the folder TCGA-A4-A4ZT-11A-01-TS1 we have 150 images.Label is: NORMAL\n",
      "In the folder TCGA-A4-A57E-11A-01-TS1 we have 373 images.Label is: NORMAL\n",
      "In the folder TCGA-B0-5097-01A-01-BS1 we have 221 images.Label is: TUMOR\n",
      "In the folder TCGA-B0-5097-01A-01-TS1 we have 79 images.Label is: TUMOR\n",
      "In the folder TCGA-B0-5113-01A-01-BS1 we have 19 images.Label is: TUMOR\n",
      "In the folder TCGA-B0-5113-01A-01-TS1 we have 23 images.Label is: TUMOR\n",
      "In the folder TCGA-B0-5712-01A-01-BS1 we have 265 images.Label is: TUMOR\n",
      "In the folder TCGA-B8-A54E-01A-01-TS1 we have 846 images.Label is: TUMOR\n",
      "In the folder TCGA-B8-A54G-01A-01-TS1 we have 481 images.Label is: TUMOR\n",
      "In the folder TCGA-B8-A54K-01A-01-TSA we have 1725 images.Label is: TUMOR\n",
      "In the folder TCGA-B9-A8YH-01A-01-TS1 we have 1553 images.Label is: TUMOR\n",
      "In the folder TCGA-BQ-5889-01A-01-BS1 we have 5001 images.Label is: TUMOR\n",
      "In the folder TCGA-BQ-5889-01A-01-TS1 we have 5001 images.Label is: TUMOR\n",
      "In the folder TCGA-BQ-7060-01A-01-TS1 we have 5001 images.Label is: TUMOR\n",
      "In the folder TCGA-CJ-4908-01A-01-BS1 we have 313 images.Label is: TUMOR\n",
      "In the folder TCGA-CJ-4908-01A-01-TS1 we have 528 images.Label is: TUMOR\n",
      "In the folder TCGA-CJ-6030-01A-01-BS1 we have 2108 images.Label is: TUMOR\n",
      "In the folder TCGA-DW-7836-01A-01-TS1 we have 863 images.Label is: TUMOR\n",
      "In the folder TCGA-G7-7502-01A-01-BS1 we have 229 images.Label is: TUMOR\n",
      "In the folder TCGA-GL-A59R-11A-01-TS1 we have 716 images.Label is: NORMAL\n",
      "In the folder TCGA-GL-A9DE-11A-01-TS1 we have 314 images.Label is: NORMAL\n",
      "In the folder TCGA-HB-A3YV-01A-01-TS1 we have 225 images.Label is: TUMOR\n",
      "In the folder TCGA-IA-A40U-01A-01-TS1 we have 2202 images.Label is: TUMOR\n",
      "In the folder TCGA-IA-A40X-01A-01-TS1 we have 2048 images.Label is: TUMOR\n",
      "In the folder TCGA-KM-A7QK-01Z-00-DX1 we have 5001 images.Label is: TUMOR\n",
      "In the folder TCGA-MH-A857-01A-01-TS1 we have 2381 images.Label is: TUMOR\n",
      "In the folder TCGA-P4-A5E6-11A-02-TS2 we have 689 images.Label is: NORMAL\n",
      "In the folder TCGA-P4-A5E7-11A-01-TSA we have 394 images.Label is: NORMAL\n",
      "In the folder TCGA-P4-A5E8-11A-01-TS1 we have 182 images.Label is: NORMAL\n",
      "In the folder TCGA-P4-A5ED-11A-01-TS1 we have 915 images.Label is: NORMAL\n",
      "In the folder TCGA-PJ-A8JU-01A-01-TSA we have 648 images.Label is: TUMOR\n",
      "In the folder TCGA-SX-A71V-01A-01-TS1 we have 914 images.Label is: TUMOR\n",
      "In the folder TCGA-SX-A7SQ-01A-01-TSA we have 1893 images.Label is: TUMOR\n",
      "In the folder TCGA-UW-A7GP-11Z-00-DX1 we have 11028 images.Label is: NORMAL\n",
      "In the folder TCGA-UW-A7GR-11Z-00-DX1 we have 5001 images.Label is: NORMAL\n",
      "In the folder TCGA-UW-A7GU-11Z-00-DX1 we have 288 images.Label is: NORMAL\n",
      "In the folder TCGA-UW-A7GX-11Z-00-DX1 we have 5001 images.Label is: NORMAL\n",
      "In the folder TCGA-UW-A7GY-11Z-00-DX1 we have 623 images.Label is: NORMAL\n",
      "In the folder TCGA-UZ-A9PR-01A-01-TSA we have 1265 images.Label is: TUMOR\n",
      "In the folder TCGA-UZ-A9PZ-01A-01-TSA we have 4084 images.Label is: TUMOR\n",
      "In the folder TCGA-UZ-A9Q0-01A-01-TSA we have 1625 images.Label is: TUMOR\n",
      "In the folder TCGA-WK-A8XT-01A-01-TS1 we have 2613 images.Label is: TUMOR\n",
      "In the folder TCGA-Y8-A897-01A-01-TS1 we have 1682 images.Label is: TUMOR\n",
      "\n",
      "\n",
      "The total number of images is: 80300\n",
      " 34 folders have label = TUMOR\n",
      " 14 folders have label = NORMAL\n",
      "The total number of TUMOR patches is 54074\n",
      "The total number of NORMAL patches is 26226\n",
      "The average number of patches for all folders is: 1672.9166666666667\n",
      "The average number of patches for TUMOR folders is: 1590.4117647058824\n",
      "The average number of patches for NORMAL folders is: 1873.2857142857142\n",
      "The patches std for all folders is: 2077.8734104180216\n",
      "The patches std for TUMOR folders is: 1542.4609084120216\n",
      "The patches std for NORMAL folders is: 2994.721056138891\n",
      "The max number of patches for all folders is: 11028\n",
      "The max number of patches for TUMOR folders is: 5001\n",
      "The max number of patches for NORMAL folders is: 11028\n",
      "The min number of patches for all folders is: 19\n",
      "The min number of patches for TUMOR folders is: 19\n",
      "The min number of patches for NORMAL folders is: 150\n"
     ]
    }
   ],
   "source": [
    "#Extract several quantites from the Cancer dataset(e.g, number of WSIs, number of tiles, min,max,avg,std of the number of tiles)\n",
    "number_of_patches = 0\n",
    "number_of_0 = 0\n",
    "number_of_1 = 0\n",
    "number_of_cancer_patches = 0\n",
    "number_of_healty_patches = 0\n",
    "patches_list = []\n",
    "negative_patches_list = []\n",
    "positive_patches_list = []\n",
    "\n",
    "j=0\n",
    "for folder in os.listdir(r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data cancer\\thesis_data_jpg' ):\n",
    "    j+=1\n",
    "    name = r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data cancer\\thesis_data_jpg' + '/' + folder\n",
    "    number_of_files = len([name for value in os.listdir(name) if os.path.isfile(name + '/' + value)])\n",
    "    label = df_cancer.loc[df_cancer['Folder name']==os.path.basename(folder)]['label'].item()\n",
    "    if label == '1':\n",
    "        number_of_1 += 1\n",
    "        number_of_cancer_patches += number_of_files\n",
    "        positive_patches_list.append(number_of_files)\n",
    "        label = 'TUMOR'\n",
    "    elif label== '0':\n",
    "        number_of_0 += 1\n",
    "        number_of_healty_patches += number_of_files\n",
    "        negative_patches_list.append(number_of_files)\n",
    "        label = 'NORMAL'\n",
    "    patches_list.append(number_of_files)\n",
    "    print(\"In the folder {} we have {} images.Label is: {}\".format(folder,number_of_files,label))\n",
    "    number_of_patches += number_of_files\n",
    "print(\"\")    \n",
    "print(\"\")    \n",
    "\n",
    "print(\"The total number of images is: {}\".format(number_of_patches))\n",
    "print(\" {} folders have label = TUMOR\".format(number_of_1))\n",
    "print(\" {} folders have label = NORMAL\".format(number_of_0))\n",
    "print(\"The total number of TUMOR patches is {}\".format(number_of_cancer_patches))\n",
    "print(\"The total number of NORMAL patches is {}\".format(number_of_healty_patches))\n",
    "\n",
    "print(\"The average number of patches for all folders is: {}\".format(np.mean(np.array(patches_list))))\n",
    "print(\"The average number of patches for TUMOR folders is: {}\".format(np.mean(np.array(positive_patches_list))))\n",
    "print(\"The average number of patches for NORMAL folders is: {}\".format(np.mean(np.array(negative_patches_list))))\n",
    "\n",
    "print(\"The patches std for all folders is: {}\".format(np.std(np.array(patches_list))))\n",
    "print(\"The patches std for TUMOR folders is: {}\".format(np.std(np.array(positive_patches_list))))\n",
    "print(\"The patches std for NORMAL folders is: {}\".format(np.std(np.array(negative_patches_list))))\n",
    "\n",
    "print(\"The max number of patches for all folders is: {}\".format(max(patches_list)))\n",
    "print(\"The max number of patches for TUMOR folders is: {}\".format(max(positive_patches_list)))\n",
    "print(\"The max number of patches for NORMAL folders is: {}\".format(max(negative_patches_list)))\n",
    "\n",
    "print(\"The min number of patches for all folders is: {}\".format(min(patches_list)))\n",
    "print(\"The min number of patches for TUMOR folders is: {}\".format(min(positive_patches_list)))\n",
    "print(\"The min number of patches for NORMAL folders is: {}\".format(min(negative_patches_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4482c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the folder TCGA-A3-3343-01A-01-BS1 we have 19 images.Label is: TIL.\n",
      "In the folder TCGA-A3-3343-01A-01-TS1 we have 29 images.Label is: TIL.\n",
      "In the folder TCGA-A3-3346-01A-01-BS1 we have 511 images.Label is: TIL.\n",
      "In the folder TCGA-A3-3346-01A-01-TS1 we have 639 images.Label is: TIL.\n",
      "In the folder TCGA-A3-3347-01A-01-TS1 we have 476 images.Label is: TIL.\n",
      "In the folder TCGA-A3-A6NL-11A-01-TS1 we have 552 images.Label is: NON-TIL.\n",
      "In the folder TCGA-A3-A8CQ-01A-01-TS1 we have 944 images.Label is: NON-TIL.\n",
      "In the folder TCGA-A4-A4ZT-01A-01-TS1 we have 1452 images.Label is: TIL.\n",
      "In the folder TCGA-A4-A4ZT-11A-01-TS1 we have 150 images.Label is: NON-TIL.\n",
      "In the folder TCGA-A4-A57E-11A-01-TS1 we have 373 images.Label is: NON-TIL.\n",
      "In the folder TCGA-A4-A7UZ-01A-01-TS1 we have 442 images.Label is: TIL.\n",
      "In the folder TCGA-AK-3425-01A-01-BS1 we have 320 images.Label is: TIL.\n",
      "In the folder TCGA-AK-3425-01A-02-BS2 we have 96 images.Label is: TIL.\n",
      "In the folder TCGA-B0-5706-01A-01-BS1 we have 293 images.Label is: TIL.\n",
      "In the folder TCGA-B2-4098-01A-02-BS2 we have 1602 images.Label is: TIL.\n",
      "In the folder TCGA-B2-4101-01A-01-BS1 we have 1042 images.Label is: TIL.\n",
      "In the folder TCGA-B4-5844-01A-01-BS1 we have 16 images.Label is: NON-TIL.\n",
      "In the folder TCGA-B8-A54E-01A-01-TS1 we have 846 images.Label is: NON-TIL.\n",
      "In the folder TCGA-B8-A54G-01A-01-TS1 we have 481 images.Label is: NON-TIL.\n",
      "In the folder TCGA-B8-A54K-01A-01-TSA we have 1725 images.Label is: NON-TIL.\n",
      "In the folder TCGA-BQ-7060-01A-01-TS1 we have 5001 images.Label is: NON-TIL.\n",
      "In the folder TCGA-CJ-6028-01A-01-BS1 we have 1593 images.Label is: TIL.\n",
      "In the folder TCGA-CJ-6028-01A-01-TS1 we have 3289 images.Label is: TIL.\n",
      "In the folder TCGA-DV-A4VX-01A-01-TSA we have 349 images.Label is: NON-TIL.\n",
      "In the folder TCGA-DV-A4W0-01A-01-TSA we have 277 images.Label is: NON-TIL.\n",
      "In the folder TCGA-DV-A4W0-05A-01-TSA we have 1153 images.Label is: NON-TIL.\n",
      "In the folder TCGA-DW-7836-01A-01-TS1 we have 863 images.Label is: NON-TIL.\n",
      "In the folder TCGA-G7-7502-01A-01-BS1 we have 229 images.Label is: NON-TIL.\n",
      "In the folder TCGA-G7-7502-01A-01-TS1 we have 1171 images.Label is: NON-TIL.\n",
      "In the folder TCGA-GL-A59R-11A-01-TS1 we have 716 images.Label is: NON-TIL.\n",
      "In the folder TCGA-GL-A59T-01A-02-TS2 we have 219 images.Label is: TIL.\n",
      "In the folder TCGA-GL-A59T-11A-01-TS1 we have 45 images.Label is: TIL.\n",
      "In the folder TCGA-GL-A59T-11A-01-TS1_1 we have 85 images.Label is: TIL.\n",
      "In the folder TCGA-GL-A9DE-11A-01-TS1 we have 314 images.Label is: NON-TIL.\n",
      "In the folder TCGA-HB-A3YV-01A-01-TS1 we have 225 images.Label is: NON-TIL.\n",
      "In the folder TCGA-IA-A40U-01A-01-TS1 we have 2202 images.Label is: NON-TIL.\n",
      "In the folder TCGA-IA-A40X-01A-01-TS1 we have 2048 images.Label is: NON-TIL.\n",
      "In the folder TCGA-IA-A40Y-01A-01-TS1 we have 3055 images.Label is: NON-TIL.\n",
      "In the folder TCGA-IA-A83T-11A-01-TS1 we have 524 images.Label is: TIL.\n",
      "In the folder TCGA-KN-8418-01A-01-BS1 we have 1370 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8418-01A-01-TS1 we have 33 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8419-01A-01-BS1 we have 5001 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8419-01A-01-TS1 we have 5001 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8421-01A-01-BS1 we have 885 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8421-01A-01-TS1 we have 2407 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8423-01A-01-BS1 we have 5001 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8423-01A-01-TS1 we have 2147 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8425-01A-01-BS1 we have 3175 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8425-01A-01-TS1 we have 1731 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8426-01A-01-BS1 we have 4473 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8426-01A-01-TS1 we have 3 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8427-01A-01-TS1 we have 3044 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8433-01A-01-BS1 we have 1369 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8433-01A-01-TS1 we have 553 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8434-01A-01-BS1 we have 1988 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8434-01A-01-TS1 we have 1065 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8437-01A-01-BS1 we have 5001 images.Label is: NON-TIL.\n",
      "In the folder TCGA-KN-8437-01A-01-TS1 we have 5001 images.Label is: NON-TIL.\n",
      "In the folder TCGA-MH-A857-01A-01-TS1 we have 2381 images.Label is: NON-TIL.\n",
      "In the folder TCGA-P4-A5E6-11A-02-TS2 we have 689 images.Label is: NON-TIL.\n",
      "In the folder TCGA-P4-A5E7-11A-01-TSA we have 394 images.Label is: NON-TIL.\n",
      "In the folder TCGA-P4-A5E8-11A-01-TS1 we have 182 images.Label is: NON-TIL.\n",
      "In the folder TCGA-P4-A5ED-11A-01-TS1 we have 915 images.Label is: NON-TIL.\n",
      "In the folder TCGA-SX-A7SQ-01A-01-TSA we have 1893 images.Label is: NON-TIL.\n",
      "In the folder TCGA-SX-A7SR-01A-01-TS1 we have 532 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A72J-01A-01-TS1 we have 373 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A72K-01A-01-TS1 we have 2598 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A72N-01A-01-TS1 we have 1717 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A72Q-01A-01-TS1 we have 2210 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A7GP-11Z-00-DX1 we have 11028 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A7GR-11Z-00-DX1 we have 5001 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A7GU-11Z-00-DX1 we have 288 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A7GX-11Z-00-DX1 we have 5001 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UW-A7GY-11Z-00-DX1 we have 623 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UZ-A9PR-01A-01-TSA we have 1265 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UZ-A9PV-01A-01-TSA we have 2724 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UZ-A9PZ-01A-01-TSA we have 4084 images.Label is: NON-TIL.\n",
      "In the folder TCGA-UZ-A9Q0-01A-01-TSA we have 1625 images.Label is: NON-TIL.\n",
      "In the folder TCGA-WK-A8XT-01A-01-TS1 we have 2613 images.Label is: NON-TIL.\n",
      "In the folder TCGA-Y8-A8RZ-11A-01-TS1 we have 208 images.Label is: TIL.\n",
      "\n",
      "\n",
      "The total number of images is: 132958\n",
      " 19 folders have label = TIL\n",
      " 61 folders have label = NON-TIL\n",
      "The total number of TIL patches is 12884\n",
      "The total number of NON-TIL patches is 120074\n",
      "The average number of patches for all folders is: 1661.975\n",
      "The average number of patches for TIL folders is: 678.1052631578947\n",
      "The average number of patches for NON-TIL folders is: 1968.4262295081967\n",
      "The patches std for all folders is: 1849.3132034285054\n",
      "The patches std for TIL folders is: 796.1761835394583\n",
      "The patches std for NON-TIL folders is: 1972.8986119622327\n",
      "The max number of patches for all folders is: 11028\n",
      "The max number of patches for TIL folders is: 3289\n",
      "The max number of patches for NON-TIL folders is: 11028\n",
      "The min number of patches for all folders is: 3\n",
      "The min number of patches for TIL folders is: 19\n",
      "The min number of patches for NON-TIL folders is: 3\n"
     ]
    }
   ],
   "source": [
    "#Count tiles and labels.\n",
    "#Count the number of patches in each folder\n",
    "number_of_patches = 0\n",
    "number_of_0 = 0\n",
    "number_of_1 = 0\n",
    "number_of_til_patches = 0\n",
    "number_of_notil_patches = 0\n",
    "patches_list = []\n",
    "negative_patches_list = []\n",
    "positive_patches_list = []\n",
    "\n",
    "j=0 \n",
    "for folder in os.listdir(r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data\\thesis_data_jpg'):\n",
    "    j+=1\n",
    "    name = r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data\\thesis_data_jpg' + '/' + folder\n",
    "    number_of_files = len([name for value in os.listdir(name) if os.path.isfile(name + '/' + value)])\n",
    "    label = df_til.loc[df_til['Folder name']==os.path.basename(folder)]['label'].item()\n",
    "    if label == 1:\n",
    "        number_of_1 += 1\n",
    "        number_of_til_patches += number_of_files\n",
    "        positive_patches_list.append(number_of_files)\n",
    "        label = 'TIL'\n",
    "    elif label== 0:\n",
    "        number_of_0 += 1\n",
    "        number_of_notil_patches += number_of_files\n",
    "        negative_patches_list.append(number_of_files)\n",
    "        label = 'NON-TIL'\n",
    "    patches_list.append(number_of_files)\n",
    "    print(\"In the folder {} we have {} images.Label is: {}.\".format(folder,number_of_files,label))\n",
    "    number_of_patches += number_of_files\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"The total number of images is: {}\".format(number_of_patches))\n",
    "print(\" {} folders have label = TIL\".format(number_of_1))\n",
    "print(\" {} folders have label = NON-TIL\".format(number_of_0))\n",
    "print(\"The total number of TIL patches is {}\".format(number_of_til_patches))\n",
    "print(\"The total number of NON-TIL patches is {}\".format(number_of_notil_patches))\n",
    "\n",
    "print(\"The average number of patches for all folders is: {}\".format(np.mean(np.array(patches_list))))\n",
    "print(\"The average number of patches for TIL folders is: {}\".format(np.mean(np.array(positive_patches_list))))\n",
    "print(\"The average number of patches for NON-TIL folders is: {}\".format(np.mean(np.array(negative_patches_list))))\n",
    "\n",
    "print(\"The patches std for all folders is: {}\".format(np.std(np.array(patches_list))))\n",
    "print(\"The patches std for TIL folders is: {}\".format(np.std(np.array(positive_patches_list))))\n",
    "print(\"The patches std for NON-TIL folders is: {}\".format(np.std(np.array(negative_patches_list))))\n",
    "\n",
    "print(\"The max number of patches for all folders is: {}\".format(max(patches_list)))\n",
    "print(\"The max number of patches for TIL folders is: {}\".format(max(positive_patches_list)))\n",
    "print(\"The max number of patches for NON-TIL folders is: {}\".format(max(negative_patches_list)))\n",
    "\n",
    "print(\"The min number of patches for all folders is: {}\".format(min(patches_list)))\n",
    "print(\"The min number of patches for TIL folders is: {}\".format(min(positive_patches_list)))\n",
    "print(\"The min number of patches for NON-TIL folders is: {}\".format(min(negative_patches_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06470e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation layers:\n",
    "#Rotation//flip\n",
    "\n",
    "augmentation = tf.keras.Sequential([\n",
    "    layers.RandomRotation(0.3,seed=42),\n",
    "    layers.RandomZoom(height_factor=(-0.3, +0.3), fill_mode = 'reflect',seed=42 ),\n",
    "    layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=42)]\n",
    ")\n",
    "\n",
    "#Resnet pretrained model:\n",
    "model = ResNet50(weights=\"imagenet\",input_shape=[224,224,3], include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts images from the folders (WSIs).\n",
    "#Augments the images with the augmentation layer.\n",
    "#Extract features from images through Resnet-50.\n",
    "#Aggregates features through Max and Mean aggregation strategies.\n",
    "#Returns two pandas dataframes with the aggregated featrues.\n",
    "\n",
    "def augmented_batch_extraction(batch_size,image_path,folders_list):\n",
    "    df_extraction_mean = pd.DataFrame([],columns=np.arange(100352))\n",
    "    df_extraction_max = pd.DataFrame([],columns=np.arange(100352))\n",
    "    \n",
    "    j=0\n",
    "\n",
    "    #for folder in os.listdir(image_path_training):\n",
    "    for folder in folders_list:\n",
    "        print(\"We are entering folder: {}\".format(folder))\n",
    "        folder_path = image_path + '\\\\' + folder \n",
    "        \n",
    "        image_folder_list = []\n",
    "        feature_list_batch = []\n",
    "        for (dirpath, dirnames, filenames) in walk(folder_path):\n",
    "            last_value = 0       \n",
    "            if batch_size <= len(filenames):\n",
    "                for _ in range(len(filenames)):\n",
    "                    imagePath = folder_path + \"\\\\\" + filenames[_]\n",
    "\n",
    "                    #Load images (and their augmentations) as arrays\n",
    "                    image = img_to_array(load_img(imagePath, target_size=(224, 224)))\n",
    "                    augmented_image = img_to_array(augmentation(image))\n",
    "                    augmented_image_copy = augmented_image.copy()\n",
    "            \n",
    "                    #preprocess images\n",
    "                    image = preprocess_input(image)\n",
    "                    augmented_image_copy = preprocess_input(augmented_image_copy)\n",
    "            \n",
    "                    #add images to folder list\n",
    "                    image_folder_list.append(image)\n",
    "                    image_folder_list.append(augmented_image_copy)\n",
    "            \n",
    "                    if (_+1) % batch_size == 0:\n",
    "                        image_array = np.array(image_folder_list[last_value:2*_+2])\n",
    "                        image_features = model.predict(image_array, batch_size=batch_size)\n",
    "                        image_features = image_features.reshape((image_features.shape[0], 7 * 7 * 2048))\n",
    "                        feature_list_batch.append(image_features)\n",
    "                        last_value = 2*_+2\n",
    "                    elif _ == len(filenames)-1:\n",
    "                        image_array = np.array(image_folder_list[last_value:2*len(filenames)])\n",
    "                        image_features = model.predict(image_array, batch_size=batch_size)\n",
    "                        image_features = image_features.reshape((image_features.shape[0], 7 * 7 * 2048))\n",
    "                        feature_list_batch.append(image_features)\n",
    "            \n",
    "            else:\n",
    "                for _ in range(len(filenames)):\n",
    "                    #Load images (and their augmentations) as arrays\n",
    "                    imagePath = folder_path + \"\\\\\" + filenames[_]\n",
    "                    image = img_to_array(load_img(imagePath, target_size=(224, 224)))\n",
    "                    augmented_image = img_to_array(augmentation(image))\n",
    "                    augmented_image_copy = augmented_image.copy()\n",
    "            \n",
    "                    #preprocess images\n",
    "                    image = preprocess_input(image)\n",
    "                    augmented_image_copy = preprocess_input(augmented_image_copy)\n",
    "            \n",
    "                    #add images to folder list\n",
    "                    image_folder_list.append(image)\n",
    "                    image_folder_list.append(augmented_image_copy)\n",
    "                \n",
    "                image_array = np.array(image_folder_list)\n",
    "                image_features = model.predict(image_array, batch_size=len(filenames))\n",
    "                image_features = image_features.reshape((image_features.shape[0], 7 * 7 * 2048))#.tolist()\n",
    "                feature_list_batch.append(image_features)\n",
    "            feature_array = np.concatenate(feature_list_batch, axis=0)\n",
    "            mean_list = np.mean(feature_array,axis=0)\n",
    "            max_list = np.max(feature_array,axis=0)\n",
    "            df_extraction_mean.loc[j] = mean_list\n",
    "            df_extraction_max.loc[j] = max_list\n",
    "\n",
    "            #data_list.append(feature_array)\n",
    "            del mean_list\n",
    "            del max_list\n",
    "            del feature_array\n",
    "        j+=1\n",
    "    return(df_extraction_mean,df_extraction_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts images from the folders (WSIs).\n",
    "#Extract features from images through Resnet-50.\n",
    "#Aggregates features through Max and Mean aggregation strategies.\n",
    "#Returns two pandas dataframes with the aggregated featrues.\n",
    "\n",
    "def batch_extraction(batch_size,image_path,folders_list):\n",
    "    df_extraction_mean = pd.DataFrame([],columns=np.arange(100352))\n",
    "    df_extraction_max = pd.DataFrame([],columns=np.arange(100352))\n",
    "\n",
    "    \n",
    "    j=0\n",
    "\n",
    "    #for folder in os.listdir(image_path_training):\n",
    "    for folder in folders_list:\n",
    "        print(\"We are entering folder: {}\".format(folder))\n",
    "        folder_path = image_path + '\\\\' + folder \n",
    "        \n",
    "        image_folder_list = []\n",
    "        feature_list_batch = []\n",
    "        for (dirpath, dirnames, filenames) in walk(folder_path):\n",
    "            #print(filenames)\n",
    "            last_value = 0       \n",
    "            if batch_size <= len(filenames):\n",
    "                #print(\"I am in the first condition!\")\n",
    "                for _ in range(len(filenames)):\n",
    "                    #Load imagesas arrays\n",
    "                    imagePath = folder_path + \"\\\\\" + filenames[_]\n",
    "                    image = img_to_array(load_img(imagePath, target_size=(224, 224)))\n",
    "            \n",
    "                    #preprocess images\n",
    "                    image = preprocess_input(image)\n",
    "            \n",
    "                    #add images to folder list\n",
    "                    image_folder_list.append(image)\n",
    "            \n",
    "                    if (_+1) % batch_size == 0:\n",
    "                        #print(_+1)\n",
    "                        image_array = np.array(image_folder_list[last_value:_+1])\n",
    "                        image_features = model.predict(image_array, batch_size=batch_size)\n",
    "                        image_features = image_features.reshape((image_features.shape[0], 7 * 7 * 2048))\n",
    "                        feature_list_batch.append(image_features)\n",
    "                        last_value = _+1\n",
    "                    elif _ == len(filenames)-1:\n",
    "                        image_array = np.array(image_folder_list[last_value:2*len(filenames)])\n",
    "                        image_features = model.predict(image_array, batch_size=batch_size)\n",
    "                        image_features = image_features.reshape((image_features.shape[0], 7 * 7 * 2048))\n",
    "                        feature_list_batch.append(image_features)          \n",
    "            else:\n",
    "                for _ in range(len(filenames)):\n",
    "                    imagePath = folder_path + \"\\\\\" + filenames[_]\n",
    "                    image = img_to_array(load_img(imagePath, target_size=(224, 224)))\n",
    "\n",
    "                    #preprocess images\n",
    "                    image = preprocess_input(image)\n",
    "            \n",
    "                    #add images to folder list\n",
    "                    image_folder_list.append(image)   \n",
    "                \n",
    "                image_array = np.array(image_folder_list)\n",
    "                image_features = model.predict(image_array, batch_size=len(filenames))\n",
    "                image_features = image_features.reshape((image_features.shape[0], 7 * 7 * 2048))\n",
    "                feature_list_batch.append(image_features)\n",
    "\n",
    "            feature_array = np.concatenate(feature_list_batch, axis=0)\n",
    "            mean_list = np.mean(feature_array,axis=0)\n",
    "            max_list = np.max(feature_array,axis=0)\n",
    "            df_extraction_mean.loc[j] = mean_list\n",
    "            df_extraction_max.loc[j] = max_list\n",
    "            del mean_list\n",
    "            del max_list\n",
    "            del feature_array\n",
    "        j+=1\n",
    "    return(df_extraction_mean,df_extraction_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression 10-fold crossvalidation\n",
    "C_list = [0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "for c in C_list:\n",
    "        print(\"c value is: {}\".format(c))\n",
    "        fig, [ax_max, ax_mean] = plt.subplots(1, 2, figsize=(18, 11))\n",
    "\n",
    "        #Initialize lists to manage metrics.\n",
    "        avg_precision_train_mean = []\n",
    "        avg_precision_test_mean = []\n",
    "\n",
    "        avg_recall_train_mean = []\n",
    "        avg_recall_test_mean = []\n",
    "\n",
    "        avg_f1_train_mean = []\n",
    "        avg_f1_test_mean = []\n",
    "\n",
    "        avg_score_train_mean = []\n",
    "        avg_score_test_mean = []\n",
    "\n",
    "        avg_precision_train_max = []\n",
    "        avg_precision_test_max = []\n",
    "\n",
    "        avg_recall_train_max = []\n",
    "        avg_recall_test_max = []\n",
    "\n",
    "        avg_f1_train_max = []\n",
    "        avg_f1_test_max = []\n",
    "        \n",
    "        avg_score_train_max = []\n",
    "        avg_score_test_max = []\n",
    "\n",
    "        #Lists for ROC curves. \n",
    "        tprs_mean = [] \n",
    "        aucs_mean = []\n",
    "\n",
    "        tprs_max = []\n",
    "        aucs_max = []\n",
    "        \n",
    "        mean_fpr_max = np.linspace(0, 1, 100)\n",
    "        mean_fpr_mean = np.linspace(0, 1, 100)\n",
    "\n",
    "        logisticRegr_mean = LogisticRegression(C = c,max_iter = 500)\n",
    "        logisticRegr_max = LogisticRegression(C = c,max_iter = 500)\n",
    "\n",
    "\n",
    "        for i in range(10):\n",
    "            _seed = i\n",
    "\n",
    "    \n",
    "            df_mean['labels'] = label_list\n",
    "            df_max['labels'] = label_list\n",
    "\n",
    "\n",
    "            y_mean = df_mean.pop('labels')\n",
    "            X_mean = df_mean\n",
    "\n",
    "            y_max = df_max.pop('labels')\n",
    "            X_max = df_max\n",
    "    \n",
    "    \n",
    "            #Create training and test sets for each fold.\n",
    "            X_train_mean_index,X_test_mean_index,y_train_mean,y_test_mean = train_test_split(X_mean.index,y_mean,random_state =_seed, test_size=0.4)\n",
    "            X_train_max_index,X_test_max_index,y_train_max,y_test_max = train_test_split(X_max.index,y_max,random_state =_seed,test_size=0.4)\n",
    "\n",
    "    \n",
    "            X_train_mean = X_mean.iloc[X_train_mean_index].to_numpy()\n",
    "            X_test_mean = X_mean.iloc[X_test_mean_index].to_numpy()\n",
    "            y_train_mean = y_train_mean.to_numpy()\n",
    "            y_test_mean = y_test_mean.to_numpy()\n",
    "    \n",
    "\n",
    "            X_train_max = X_max.iloc[X_train_max_index].to_numpy()\n",
    "            X_test_max = X_max.iloc[X_test_max_index].to_numpy()\n",
    "            y_train_max = y_train_max.to_numpy()\n",
    "            y_test_max = y_test_max.to_numpy()\n",
    "        \n",
    "    \n",
    "            #MEAN AGGREGATION MODEL: perform logistic regression on mean-aggregated data.\n",
    "            logisticRegr_mean.fit(X_train_mean, y_train_mean)\n",
    "            y_score = logisticRegr_mean.fit(X_train_mean, y_train_mean).decision_function(X_test_mean)\n",
    "            predictions_train_mean = logisticRegr_mean.predict(X_train_mean)\n",
    "            predictions_test_mean = logisticRegr_mean.predict(X_test_mean)\n",
    "    \n",
    "            score_training_mean = logisticRegr_mean.score(X_train_mean, y_train_mean)\n",
    "            f1_training_mean = f1_score(y_train_mean, predictions_train_mean, average='binary')\n",
    "            precision_training_mean = precision_score(y_train_mean, predictions_train_mean, average='binary')\n",
    "            recall_training_mean = recall_score(y_train_mean, predictions_train_mean, average='binary')\n",
    "    \n",
    "            score_test_mean = logisticRegr_mean.score(X_test_mean, y_test_mean)\n",
    "            f1_test_mean = f1_score(y_test_mean, predictions_test_mean, average='binary')\n",
    "            precision_test_mean =precision_score(y_test_mean, predictions_test_mean, average='binary')\n",
    "            recall_test_mean = recall_score(y_test_mean, predictions_test_mean, average='binary')\n",
    "\n",
    "            avg_precision_train_mean.append(precision_training_mean)\n",
    "            avg_precision_test_mean.append(precision_test_mean)\n",
    "\n",
    "            avg_recall_train_mean.append(recall_training_mean)\n",
    "            avg_recall_test_mean.append(recall_test_mean)\n",
    "\n",
    "            avg_f1_train_mean.append(f1_training_mean)\n",
    "            avg_f1_test_mean.append(f1_test_mean)\n",
    "\n",
    "            avg_score_train_mean.append(score_training_mean)\n",
    "            avg_score_test_mean.append(score_test_mean)\n",
    "    \n",
    "            #collect data for mean aggregation ROC curve\n",
    "            viz_mean = RocCurveDisplay.from_estimator(\n",
    "                logisticRegr_mean,\n",
    "                X_test_mean,\n",
    "                y_test_mean,\n",
    "                ax=ax_mean,\n",
    "                name=\"ROC fold {}\".format(i+1),\n",
    "                alpha=0.3,\n",
    "            )\n",
    "    \n",
    "            interp_tpr_avg = np.interp(mean_fpr_mean, viz_mean.fpr, viz_mean.tpr)\n",
    "            interp_tpr_avg[0] = 0.0\n",
    "            tprs_mean.append(interp_tpr_avg)\n",
    "            aucs_mean.append(viz_mean.roc_auc)\n",
    "    \n",
    "    \n",
    "            #MAX AGGREGATION MODEL: perform logistic regression on max-aggregated data.\n",
    "            logisticRegr_max.fit(X_train_max, y_train_max)\n",
    "            y_score = logisticRegr_max.fit(X_train_max, y_train_max).decision_function(X_test_max)\n",
    "            predictions_train_max = logisticRegr_max.predict(X_train_max)\n",
    "            predictions_test_max = logisticRegr_max.predict(X_test_max)\n",
    "\n",
    "            score_training_max = logisticRegr_max.score(X_train_max, y_train_max)\n",
    "            f1_training_max = f1_score(y_train_max, predictions_train_max, average='binary')\n",
    "            precision_training_max = precision_score(y_train_max, predictions_train_max, average='binary')\n",
    "            recall_training_max = recall_score(y_train_max, predictions_train_max, average='binary')\n",
    "\n",
    "            score_test_max = logisticRegr_max.score(X_test_max, y_test_max)\n",
    "            f1_test_max = f1_score(y_test_max, predictions_test_max, average='binary')\n",
    "            precision_test_max =precision_score(y_test_max, predictions_test_max, average='binary')\n",
    "            recall_test_max = recall_score(y_test_max, predictions_test_max, average='binary')\n",
    "\n",
    "            avg_precision_train_max.append(precision_training_max)\n",
    "            avg_precision_test_max.append(precision_test_max)\n",
    "\n",
    "            avg_recall_train_max.append(recall_training_max)\n",
    "            avg_recall_test_max.append(recall_test_max)\n",
    "\n",
    "            avg_f1_train_max.append(f1_training_max)\n",
    "            avg_f1_test_max.append(f1_test_max)\n",
    "\n",
    "            avg_score_train_max.append(score_training_max)\n",
    "            avg_score_test_max.append(score_test_max)\n",
    "\n",
    "\n",
    "            #collect data for max aggregation ROC curve\n",
    "            viz_max = RocCurveDisplay.from_estimator(\n",
    "                logisticRegr_max,\n",
    "                X_test_max,\n",
    "                y_test_max,\n",
    "                ax=ax_max,\n",
    "                name=\"ROC fold {}\".format(i+1),\n",
    "                alpha=0.3,\n",
    "            )\n",
    "    \n",
    "\n",
    "            interp_tpr_max = np.interp(mean_fpr_max, viz_max.fpr, viz_max.tpr)\n",
    "            interp_tpr_max[0] = 0.0\n",
    "            tprs_max.append(interp_tpr_max)\n",
    "            aucs_max.append(viz_max.roc_auc)\n",
    "    \n",
    "    \n",
    "\n",
    "        #Plot avg ROC curves (max aggregation)\n",
    "        mean_tpr_max = np.mean(tprs_max, axis=0)\n",
    "        mean_tpr_max[-1] = 1.0\n",
    "        mean_auc_max = auc(mean_fpr_max, mean_tpr_max)\n",
    "        std_auc_max = np.std(aucs_max)\n",
    "\n",
    "        ax_max.plot(\n",
    "            mean_fpr_max,\n",
    "            mean_tpr_max,\n",
    "            color=\"b\",\n",
    "            label=r\"Max ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_max, std_auc_max),\n",
    "            lw=2,\n",
    "            alpha=0.8,\n",
    "        )    \n",
    "\n",
    "        ax_max.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)    \n",
    "\n",
    "        ax_max.set(\n",
    "            xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic example MAX\",\n",
    "        )\n",
    "        ax_max.legend(loc=\"lower right\")\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "\n",
    "        #Plot avg ROC curves (mean aggregation)\n",
    "        mean_tpr_mean = np.mean(tprs_mean, axis=0)\n",
    "        mean_tpr_mean[-1] = 1.0\n",
    "        mean_auc_mean = auc(mean_fpr_mean, mean_tpr_mean)\n",
    "        std_auc_mean = np.std(aucs_mean)\n",
    "\n",
    "        ax_mean.plot(\n",
    "            mean_fpr_mean,\n",
    "            mean_tpr_mean,\n",
    "            color=\"b\",\n",
    "            label=r\"Max ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_mean, std_auc_mean),\n",
    "            lw=2,\n",
    "            alpha=0.8,\n",
    "        )    \n",
    "\n",
    "        ax_mean.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)    \n",
    "\n",
    "        ax_mean.set(\n",
    "            xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic example MAX\",\n",
    "        )\n",
    "        ax_mean.legend(loc=\"lower right\")\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "\n",
    "\n",
    "        ax_max.set_title(\"Receiver Operating Characteristic Curves, Max aggregation, Log. Regression.\")\n",
    "        ax_mean.set_title(\"Receiver Operating Characteristic Curves, Mean aggregation, Log. Regression.\")\n",
    "\n",
    "        ax_max.grid(linestyle=\"--\")\n",
    "        ax_mean.grid(linestyle=\"--\")\n",
    "\n",
    "        ########################################\n",
    "        ########################################\n",
    "        print(\"Avg score training sets mean: {}\".format(np.mean(np.array(avg_score_train_mean))))\n",
    "        print(\"Avg score test sets mean: {}\".format(np.mean(np.array(avg_score_test_mean))))\n",
    "\n",
    "        print(\"Avg precision training sets mean: {}\".format(np.mean(np.array(avg_precision_train_mean))))\n",
    "        print(\"Avg precision test sets mean: {}\".format(np.mean(np.array(avg_precision_test_mean))))\n",
    "\n",
    "        print(\"Avg recall training sets mean: {}\".format(np.mean(np.array(avg_recall_train_mean))))\n",
    "        print(\"Avg recall test sets mean: {}\".format(np.mean(np.array(avg_recall_test_mean))))\n",
    "\n",
    "        print(\"Avg F1 training sets mean: {}\".format(np.mean(np.array(avg_f1_train_mean))))\n",
    "        print(\"Avg F1 test sets mean: {}\".format(np.mean(np.array(avg_f1_test_mean))))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Std score training sets mean: {}\".format(np.std(np.array(avg_score_train_mean))))\n",
    "        print(\"Std score test sets mean: {}\".format(np.std(np.array(avg_score_test_mean))))\n",
    "\n",
    "        print(\"Std precision training sets mean: {}\".format(np.std(np.array(avg_precision_train_mean))))\n",
    "        print(\"Std precision test sets mean: {}\".format(np.std(np.array(avg_precision_test_mean))))\n",
    "\n",
    "        print(\"Std recall training sets mean: {}\".format(np.std(np.array(avg_recall_train_mean))))\n",
    "        print(\"Std recall test sets mean: {}\".format(np.std(np.array(avg_recall_test_mean))))\n",
    "\n",
    "        print(\"Std F1 training sets mean: {}\".format(np.std(np.array(avg_f1_train_mean))))\n",
    "        print(\"Std F1 test sets mean: {}\".format(np.std(np.array(avg_f1_test_mean))))\n",
    "        #########################################\n",
    "        #########################################\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        ########################################\n",
    "        ########################################\n",
    "        print(\"\")\n",
    "        print(\"Avg score training sets max: {}\".format(np.mean(np.array(avg_score_train_max))))\n",
    "        print(\"Avg score test sets max: {}\".format(np.mean(np.array(avg_score_test_max))))\n",
    "\n",
    "        print(\"Avg precision training sets max: {}\".format(np.mean(np.array(avg_precision_train_max))))\n",
    "        print(\"Avg precision test sets max: {}\".format(np.mean(np.array(avg_precision_test_max))))\n",
    "\n",
    "        print(\"Avg recall training sets max: {}\".format(np.mean(np.array(avg_recall_train_max))))\n",
    "        print(\"Avg recall test sets max: {}\".format(np.mean(np.array(avg_recall_test_max))))\n",
    "\n",
    "        print(\"Avg F1 training sets max: {}\".format(np.mean(np.array(avg_f1_train_max))))\n",
    "        print(\"Avg F1 test sets max: {}\".format(np.mean(np.array(avg_f1_test_max))))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Std score training sets max: {}\".format(np.std(np.array(avg_score_train_max))))\n",
    "        print(\"Std score test sets max: {}\".format(np.std(np.array(avg_score_test_max))))\n",
    "\n",
    "        print(\"Std precision training sets max: {}\".format(np.std(np.array(avg_precision_train_max))))\n",
    "        print(\"Std precision test sets max: {}\".format(np.std(np.array(avg_precision_test_max))))\n",
    "\n",
    "        print(\"Std recall training sets max: {}\".format(np.std(np.array(avg_recall_train_max))))\n",
    "        print(\"Std recall test sets max: {}\".format(np.std(np.array(avg_recall_test_max))))\n",
    "\n",
    "        print(\"Std F1 training sets max: {}\".format(np.std(np.array(avg_f1_train_max))))\n",
    "        print(\"Std F1 test sets max: {}\".format(np.std(np.array(avg_f1_test_max))))\n",
    "\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "for c in C_list:\n",
    "        print(\"c value is: {}\".format(c))\n",
    "        print(\"gama value is: {}\".format(gamma))\n",
    "        #Training test with support vecto machine.\n",
    "        fig, [ax_max, ax_mean] = plt.subplots(1, 2, figsize=(18, 11))\n",
    "\n",
    "        #Initialize lists to manage metrics.\n",
    "        avg_precision_train_mean = []\n",
    "        avg_precision_test_mean = []\n",
    "\n",
    "        avg_recall_train_mean = []\n",
    "        avg_recall_test_mean = []\n",
    "\n",
    "        avg_f1_train_mean = []\n",
    "        avg_f1_test_mean = []\n",
    "\n",
    "        avg_score_train_mean = []\n",
    "        avg_score_test_mean = []\n",
    "\n",
    "        avg_precision_train_max = []\n",
    "        avg_precision_test_max = []\n",
    "\n",
    "        avg_recall_train_max = []\n",
    "        avg_recall_test_max = []\n",
    "\n",
    "        avg_f1_train_max = []\n",
    "        avg_f1_test_max = []\n",
    "        \n",
    "        avg_score_train_max = []\n",
    "        avg_score_test_max = []\n",
    "\n",
    "        \n",
    "        \n",
    "        #Lists for ROC curves. \n",
    "        tprs_mean = [] \n",
    "        aucs_mean = []\n",
    "\n",
    "        tprs_max = []\n",
    "        aucs_max = []\n",
    "        mean_fpr_max = np.linspace(0, 1, 100)\n",
    "        mean_fpr_mean = np.linspace(0, 1, 100)\n",
    "        \n",
    "        \n",
    "        svm_mean = svm.SVC(C = c, gamma = gamma, max_iter = 500)\n",
    "        svm_max = svm.SVC(C = c, gamma = gamma, max_iter = 500)\n",
    "\n",
    "\n",
    "        for i in range(10):\n",
    "            _seed = i\n",
    "    \n",
    "            df_mean['labels'] = label_list\n",
    "            df_max['labels'] = label_list\n",
    "\n",
    "\n",
    "            y_mean = df_mean.pop('labels')\n",
    "            X_mean = df_mean\n",
    "\n",
    "            y_max = df_max.pop('labels')\n",
    "            X_max = df_max\n",
    "    \n",
    "    \n",
    "            #Create training and test sets for each fold.\n",
    "            X_train_mean_index,X_test_mean_index,y_train_mean,y_test_mean = train_test_split(X_mean.index,y_mean,random_state =_seed, test_size=0.4)\n",
    "            X_train_max_index,X_test_max_index,y_train_max,y_test_max = train_test_split(X_max.index,y_max,random_state =_seed,test_size=0.4)\n",
    "   \n",
    "            X_train_mean = X_mean.iloc[X_train_mean_index].to_numpy()\n",
    "            X_test_mean = X_mean.iloc[X_test_mean_index].to_numpy()\n",
    "            y_train_mean = y_train_mean.to_numpy()\n",
    "            y_test_mean = y_test_mean.to_numpy()\n",
    "    \n",
    "\n",
    "            X_train_max = X_max.iloc[X_train_max_index].to_numpy()\n",
    "            X_test_max = X_max.iloc[X_test_max_index].to_numpy()\n",
    "            y_train_max = y_train_max.to_numpy()\n",
    "            y_test_max = y_test_max.to_numpy()\n",
    "        \n",
    "    \n",
    "            #MEAN AGGREGATION MODEL: perform svm on mean-aggregated data.\n",
    "            svm_mean.fit(X_train_mean, y_train_mean)\n",
    "            y_score = svm_mean.fit(X_train_mean, y_train_mean).decision_function(X_test_mean)\n",
    "            predictions_train_mean = svm_mean.predict(X_train_mean)\n",
    "            predictions_test_mean = svm_mean.predict(X_test_mean)\n",
    "    \n",
    "            score_training_mean = svm_mean.score(X_train_mean, y_train_mean)\n",
    "            f1_training_mean = f1_score(y_train_mean, predictions_train_mean, average='binary')\n",
    "            precision_training_mean = precision_score(y_train_mean, predictions_train_mean, average='binary')\n",
    "            recall_training_mean = recall_score(y_train_mean, predictions_train_mean, average='binary')\n",
    "    \n",
    "            score_test_mean = svm_mean.score(X_test_mean, y_test_mean)\n",
    "            f1_test_mean = f1_score(y_test_mean, predictions_test_mean, average='binary')\n",
    "            precision_test_mean =precision_score(y_test_mean, predictions_test_mean, average='binary')\n",
    "            recall_test_mean = recall_score(y_test_mean, predictions_test_mean, average='binary')\n",
    "        \n",
    "            avg_precision_train_mean.append(precision_training_mean)\n",
    "            avg_precision_test_mean.append(precision_test_mean)\n",
    "\n",
    "            avg_recall_train_mean.append(recall_training_mean)\n",
    "            avg_recall_test_mean.append(recall_test_mean)\n",
    "\n",
    "            avg_f1_train_mean.append(f1_training_mean)\n",
    "            avg_f1_test_mean.append(f1_test_mean)\n",
    "\n",
    "            avg_score_train_mean.append(score_training_mean)\n",
    "            avg_score_test_mean.append(score_test_mean)\n",
    "    \n",
    "            #collect data for mean aggregation ROC curve\n",
    "            viz_mean = RocCurveDisplay.from_estimator(\n",
    "                svm_mean,\n",
    "                X_test_mean,\n",
    "                y_test_mean,\n",
    "                ax=ax_mean,\n",
    "                name=\"ROC fold {}\".format(i+1),\n",
    "                alpha=0.3,\n",
    "            )\n",
    "    \n",
    "            interp_tpr_avg = np.interp(mean_fpr_mean, viz_mean.fpr, viz_mean.tpr)\n",
    "            interp_tpr_avg[0] = 0.0\n",
    "            tprs_mean.append(interp_tpr_avg)\n",
    "            aucs_mean.append(viz_mean.roc_auc)\n",
    "    \n",
    "    \n",
    "            #MAX AGGREGATION MODEL: perform svm on mean-aggregated data.\n",
    "            svm_max.fit(X_train_max, y_train_max)\n",
    "            y_score = svm_max.fit(X_train_max, y_train_max).decision_function(X_test_max)\n",
    "            predictions_train_max = svm_max.predict(X_train_max)\n",
    "            predictions_test_max = svm_max.predict(X_test_max)\n",
    "\n",
    "            score_training_max = svm_max.score(X_train_max, y_train_max)\n",
    "            f1_training_max = f1_score(y_train_max, predictions_train_max, average='binary')\n",
    "            precision_training_max = precision_score(y_train_max, predictions_train_max, average='binary')\n",
    "            recall_training_max = recall_score(y_train_max, predictions_train_max, average='binary')\n",
    "\n",
    "            score_test_max = svm_max.score(X_test_max, y_test_max)\n",
    "            f1_test_max = f1_score(y_test_max, predictions_test_max, average='binary')\n",
    "            precision_test_max =precision_score(y_test_max, predictions_test_max, average='binary')\n",
    "            recall_test_max = recall_score(y_test_max, predictions_test_max, average='binary')\n",
    "\n",
    "            avg_precision_train_max.append(precision_training_max)\n",
    "            avg_precision_test_max.append(precision_test_max)\n",
    "\n",
    "            avg_recall_train_max.append(recall_training_max)\n",
    "            avg_recall_test_max.append(recall_test_max)\n",
    "\n",
    "            avg_f1_train_max.append(f1_training_max)\n",
    "            avg_f1_test_max.append(f1_test_max)\n",
    "\n",
    "            avg_score_train_max.append(score_training_max)\n",
    "            avg_score_test_max.append(score_test_max)\n",
    "\n",
    "\n",
    "            #collect data for max aggregation ROC curve\n",
    "            viz_max = RocCurveDisplay.from_estimator(\n",
    "                svm_max,\n",
    "                X_test_max,\n",
    "                y_test_max,\n",
    "                ax=ax_max,\n",
    "                name=\"ROC fold {}\".format(i+1),\n",
    "                alpha=0.3,\n",
    "            )\n",
    "    \n",
    "\n",
    "            interp_tpr_max = np.interp(mean_fpr_max, viz_max.fpr, viz_max.tpr)\n",
    "            interp_tpr_max[0] = 0.0\n",
    "            tprs_max.append(interp_tpr_max)\n",
    "            aucs_max.append(viz_max.roc_auc)\n",
    "    \n",
    "    \n",
    "        ############################################    \n",
    "        ############################################\n",
    "        ############################################\n",
    "        #Plot avg ROC curves (max aggregation)\n",
    "        mean_tpr_max = np.mean(tprs_max, axis=0)\n",
    "        mean_tpr_max[-1] = 1.0\n",
    "        mean_auc_max = auc(mean_fpr_max, mean_tpr_max)\n",
    "        std_auc_max = np.std(aucs_max)\n",
    "\n",
    "        ax_max.plot(\n",
    "            mean_fpr_max,\n",
    "            mean_tpr_max,\n",
    "            color=\"b\",\n",
    "            label=r\"Max ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_max, std_auc_max),\n",
    "            lw=2,\n",
    "            alpha=0.8,\n",
    "        )    \n",
    "\n",
    "        ax_max.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)    \n",
    "\n",
    "        ax_max.set(\n",
    "            xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic example MAX\",\n",
    "        )\n",
    "        ax_max.legend(loc=\"lower right\")\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "        ###################\n",
    "        ##################\n",
    "        #Plot avg ROC curves (mean aggregation)\n",
    "        mean_tpr_mean = np.mean(tprs_mean, axis=0)\n",
    "        mean_tpr_mean[-1] = 1.0\n",
    "        mean_auc_mean = auc(mean_fpr_mean, mean_tpr_mean)\n",
    "        std_auc_mean = np.std(aucs_mean)\n",
    "\n",
    "        ax_mean.plot(\n",
    "            mean_fpr_mean,\n",
    "            mean_tpr_mean,\n",
    "            color=\"b\",\n",
    "            label=r\"Max ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_mean, std_auc_mean),\n",
    "            lw=2,\n",
    "            alpha=0.8,\n",
    "        )    \n",
    "\n",
    "        ax_mean.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)    \n",
    "\n",
    "        ax_mean.set(\n",
    "            xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic example MAX\",\n",
    "        )\n",
    "        ax_mean.legend(loc=\"lower right\")\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "        ax_max.set_title(\"Receiver Operating Characteristic Curves, Max aggregation.\")\n",
    "        ax_mean.set_title(\"Receiver Operating Characteristic Curves, Mean aggregation.\")\n",
    "\n",
    "        ax_max.grid(linestyle=\"--\")\n",
    "        ax_mean.grid(linestyle=\"--\")\n",
    "\n",
    "        ########################################\n",
    "        ########################################\n",
    "        print(\"Avg score training sets mean: {}\".format(np.mean(np.array(avg_score_train_mean))))\n",
    "        print(\"Avg score test sets mean: {}\".format(np.mean(np.array(avg_score_test_mean))))\n",
    "\n",
    "        print(\"Avg precision training sets mean: {}\".format(np.mean(np.array(avg_precision_train_mean))))\n",
    "        print(\"Avg precision test sets mean: {}\".format(np.mean(np.array(avg_precision_test_mean))))\n",
    "\n",
    "        print(\"Avg recall training sets mean: {}\".format(np.mean(np.array(avg_recall_train_mean))))\n",
    "        print(\"Avg recall test sets mean: {}\".format(np.mean(np.array(avg_recall_test_mean))))\n",
    "\n",
    "        print(\"Avg F1 training sets mean: {}\".format(np.mean(np.array(avg_f1_train_mean))))\n",
    "        print(\"Avg F1 test sets mean: {}\".format(np.mean(np.array(avg_f1_test_mean))))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Std score training sets mean: {}\".format(np.std(np.array(avg_score_train_mean))))\n",
    "        print(\"Std score test sets mean: {}\".format(np.std(np.array(avg_score_test_mean))))\n",
    "\n",
    "        print(\"Std precision training sets mean: {}\".format(np.std(np.array(avg_precision_train_mean))))\n",
    "        print(\"Std precision test sets mean: {}\".format(np.std(np.array(avg_precision_test_mean))))\n",
    "\n",
    "        print(\"Std recall training sets mean: {}\".format(np.std(np.array(avg_recall_train_mean))))\n",
    "        print(\"Std recall test sets mean: {}\".format(np.std(np.array(avg_recall_test_mean))))\n",
    "\n",
    "        print(\"Std F1 training sets mean: {}\".format(np.std(np.array(avg_f1_train_mean))))\n",
    "        print(\"Std F1 test sets mean: {}\".format(np.std(np.array(avg_f1_test_mean))))\n",
    "        #########################################\n",
    "        #########################################\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        ########################################\n",
    "        ########################################\n",
    "        print(\"\")\n",
    "        print(\"Avg score training sets max: {}\".format(np.mean(np.array(avg_score_train_max))))\n",
    "        print(\"Avg score test sets max: {}\".format(np.mean(np.array(avg_score_test_max))))\n",
    "\n",
    "        print(\"Avg precision training sets max: {}\".format(np.mean(np.array(avg_precision_train_max))))\n",
    "        print(\"Avg precision test sets max: {}\".format(np.mean(np.array(avg_precision_test_max))))\n",
    "\n",
    "        print(\"Avg recall training sets max: {}\".format(np.mean(np.array(avg_recall_train_max))))\n",
    "        print(\"Avg recall test sets max: {}\".format(np.mean(np.array(avg_recall_test_max))))\n",
    "\n",
    "        print(\"Avg F1 training sets max: {}\".format(np.mean(np.array(avg_f1_train_max))))\n",
    "        print(\"Avg F1 test sets max: {}\".format(np.mean(np.array(avg_f1_test_max))))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Std score training sets max: {}\".format(np.std(np.array(avg_score_train_max))))\n",
    "        print(\"Std score test sets max: {}\".format(np.std(np.array(avg_score_test_max))))\n",
    "\n",
    "        print(\"Std precision training sets max: {}\".format(np.std(np.array(avg_precision_train_max))))\n",
    "        print(\"Std precision test sets max: {}\".format(np.std(np.array(avg_precision_test_max))))\n",
    "\n",
    "        print(\"Std recall training sets max: {}\".format(np.std(np.array(avg_recall_train_max))))\n",
    "        print(\"Std recall test sets max: {}\".format(np.std(np.array(avg_recall_test_max))))\n",
    "\n",
    "        print(\"Std F1 training sets max: {}\".format(np.std(np.array(avg_f1_train_max))))\n",
    "        print(\"Std F1 test sets max: {}\".format(np.std(np.array(avg_f1_test_max))))\n",
    "\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a list with the possibile permutations of two layers:\n",
    "#minimum number of nodes per layer: 32\n",
    "#maximum number of nodes per layer: 224\n",
    "#step size is 32.\n",
    "\n",
    "num_layers = 2\n",
    "min_nodes_per_layer = 32\n",
    "max_nodes_per_layer = 224\n",
    "node_step_size = 32\n",
    "\n",
    "node_options = list(range(\n",
    "    min_nodes_per_layer, \n",
    "    max_nodes_per_layer + 1, \n",
    "    node_step_size\n",
    "))\n",
    "\n",
    "two_layer_possibilities = [node_options, node_options]\n",
    "\n",
    "layer_possibilities = [node_options] * num_layers\n",
    "layer_node_permutations = list(itertools.product(*layer_possibilities))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "\n",
    "for permutation in layer_node_permutations:\n",
    "    \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        #Build model architecture and print it\n",
    "        print(\"permutation is: {}\".format(permutation))\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.Input(shape=(7 * 7 * 2048,)))\n",
    "        \n",
    "        for nodes_at_layer in permutation:\n",
    "            model.add(tf.keras.layers.Dense(nodes_at_layer, activation='relu'))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        models.append(model)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #Training test with MLP.\n",
    "        fig, [ax_max, ax_mean] = plt.subplots(1, 2, figsize=(18, 11))\n",
    "\n",
    "        #Initialize lists to manage metrics.\n",
    "        avg_precision_train_mean = []\n",
    "        avg_precision_test_mean = []\n",
    "\n",
    "        avg_recall_train_mean = []\n",
    "        avg_recall_test_mean = []\n",
    "\n",
    "        avg_f1_train_mean = []\n",
    "        avg_f1_test_mean = []\n",
    "\n",
    "        avg_score_train_mean = []\n",
    "        avg_score_test_mean = []\n",
    "\n",
    "        avg_precision_train_max = []\n",
    "        avg_precision_test_max = []\n",
    "\n",
    "        avg_recall_train_max = []\n",
    "        avg_recall_test_max = []\n",
    "\n",
    "        avg_f1_train_max = []\n",
    "        avg_f1_test_max = []\n",
    "        \n",
    "        avg_score_train_max = []\n",
    "        avg_score_test_max = []\n",
    "\n",
    "\n",
    "        #Lists for ROC curves. \n",
    "        tprs_mean = [] \n",
    "        aucs_mean = []\n",
    "\n",
    "        tprs_max = []\n",
    "        aucs_max = []\n",
    "        mean_fpr_max = np.linspace(0, 1, 100)\n",
    "        mean_fpr_mean = np.linspace(0, 1, 100)\n",
    "        \n",
    "        \n",
    "        for i in range(10):\n",
    "            \n",
    "            print(\"i value is: {}\".format(i))\n",
    "            _seed = i\n",
    "\n",
    "    \n",
    "            df_mean['labels'] = label_list\n",
    "            df_max['labels'] = label_list\n",
    "\n",
    "\n",
    "            y_mean = df_mean.pop('labels')\n",
    "            X_mean = df_mean\n",
    "\n",
    "            y_max = df_max.pop('labels')\n",
    "            X_max = df_max\n",
    "    \n",
    "    \n",
    "            #Create training and test sets for each fold.\n",
    "            X_train_mean_index,X_test_mean_index,y_train_mean,y_test_mean = train_test_split(X_mean.index,y_mean,random_state =_seed, test_size=0.4)\n",
    "            X_train_max_index,X_test_max_index,y_train_max,y_test_max = train_test_split(X_max.index,y_max,random_state =_seed,test_size=0.4)\n",
    "    \n",
    "            X_train_mean = X_mean.iloc[X_train_mean_index].to_numpy()\n",
    "            X_test_mean = X_mean.iloc[X_test_mean_index].to_numpy()\n",
    "            y_train_mean = y_train_mean.to_numpy()\n",
    "            y_test_mean = y_test_mean.to_numpy()\n",
    "    \n",
    "\n",
    "            X_train_max = X_max.iloc[X_train_max_index].to_numpy()\n",
    "            X_test_max = X_max.iloc[X_test_max_index].to_numpy()\n",
    "            y_train_max = y_train_max.to_numpy()\n",
    "            y_test_max = y_test_max.to_numpy()\n",
    "    \n",
    "            #MEAN AGGREGATION MODEL: perform mlp on mean-aggregated data.\n",
    "            model.fit(X_train_mean, y_train_mean, epochs=500,verbose = 0)\n",
    "\n",
    "            predictions_train_mean = model.predict(X_train_mean,batch_size=64, verbose=0)\n",
    "            predictions_test_mean = model.predict(X_test_mean,batch_size=64, verbose=0)\n",
    "    \n",
    "    \n",
    "            score_training_mean = accuracy_score(y_train_mean, (predictions_train_mean > 0.5).astype(\"int32\"))\n",
    "            f1_training_mean = f1_score(y_train_mean, (predictions_train_mean > 0.5).astype(\"int32\"), average='binary')\n",
    "            precision_training_mean = precision_score(y_train_mean, (predictions_train_mean > 0.5).astype(\"int32\"), average='binary')\n",
    "            recall_training_mean = recall_score(y_train_mean, (predictions_train_mean > 0.5).astype(\"int32\"), average='binary')\n",
    "    \n",
    "            score_test_mean = accuracy_score(y_test_mean, (predictions_test_mean > 0.5).astype(\"int32\"))\n",
    "            f1_test_mean = f1_score(y_test_mean, (predictions_test_mean > 0.5).astype(\"int32\"), average='binary')\n",
    "            precision_test_mean =precision_score(y_test_mean, (predictions_test_mean > 0.5).astype(\"int32\"), average='binary')\n",
    "            recall_test_mean = recall_score(y_test_mean, (predictions_test_mean > 0.5).astype(\"int32\"), average='binary')\n",
    "\n",
    "            avg_f1_values_mean.append([f1_training_mean,f1_test_mean,permutation])\n",
    "\n",
    "            \n",
    "            avg_precision_train_mean.append(precision_training_mean)\n",
    "            avg_precision_test_mean.append(precision_test_mean)\n",
    "\n",
    "            avg_recall_train_mean.append(recall_training_mean)\n",
    "            avg_recall_test_mean.append(recall_test_mean)\n",
    "\n",
    "            avg_f1_train_mean.append(f1_training_mean)\n",
    "            avg_f1_test_mean.append(f1_test_mean)\n",
    "\n",
    "            avg_score_train_mean.append(score_training_mean)\n",
    "            avg_score_test_mean.append(score_test_mean)\n",
    "    \n",
    "            #collect data for mean aggregation ROC curve\n",
    "            viz_mean = RocCurveDisplay.from_predictions(\n",
    "                y_test_mean,\n",
    "                (predictions_test_mean > 0.5).astype(\"int32\"),\n",
    "                ax=ax_mean,\n",
    "                name=\"ROC fold {}\".format(i+1),\n",
    "                alpha=0.3\n",
    "            )\n",
    "            \n",
    "            interp_tpr_avg = np.interp(mean_fpr_mean, viz_mean.fpr, viz_mean.tpr)\n",
    "            interp_tpr_avg[0] = 0.0\n",
    "            tprs_mean.append(interp_tpr_avg)\n",
    "            aucs_mean.append(viz_mean.roc_auc)\n",
    "    \n",
    "    \n",
    "            #MAX AGGREGATION MODEL: perform mlp on max-aggregated data.\n",
    "            model.fit(X_train_max, y_train_max, epochs=500,verbose = 0)\n",
    "            \n",
    "            predictions_train_max = model.predict(X_train_max,batch_size=64, verbose=0)\n",
    "            predictions_test_max = model.predict(X_test_max,batch_size=64, verbose=0)\n",
    "\n",
    "            score_training_max = accuracy_score(y_train_max, (predictions_train_max > 0.5).astype(\"int32\"))\n",
    "            f1_training_max = f1_score(y_train_max, (predictions_train_max > 0.5).astype(\"int32\"), average='binary')\n",
    "            precision_training_max = precision_score(y_train_max, (predictions_train_max > 0.5).astype(\"int32\"), average='binary')\n",
    "            recall_training_max = recall_score(y_train_max, (predictions_train_max > 0.5).astype(\"int32\"), average='binary')\n",
    "\n",
    "            score_test_max = accuracy_score(y_test_max, (predictions_test_max > 0.5).astype(\"int32\"))\n",
    "            f1_test_max = f1_score(y_test_max, (predictions_test_max > 0.5).astype(\"int32\"), average='binary')\n",
    "            precision_test_max =precision_score(y_test_max, (predictions_test_max > 0.5).astype(\"int32\"), average='binary')\n",
    "            recall_test_max = recall_score(y_test_max, (predictions_test_max > 0.5).astype(\"int32\"), average='binary')\n",
    "            \n",
    "\n",
    "            avg_precision_train_max.append(precision_training_max)\n",
    "            avg_precision_test_max.append(precision_test_max)\n",
    "\n",
    "            avg_recall_train_max.append(recall_training_max)\n",
    "            avg_recall_test_max.append(recall_test_max)\n",
    "\n",
    "            avg_f1_train_max.append(f1_training_max)\n",
    "            avg_f1_test_max.append(f1_test_max)\n",
    "\n",
    "            avg_score_train_max.append(score_training_max)\n",
    "            avg_score_test_max.append(score_test_max)\n",
    "\n",
    "            \n",
    "\n",
    "            #collect data for max aggregation ROC curve\n",
    "            viz_max = RocCurveDisplay.from_predictions(\n",
    "                    y_test_max,\n",
    "                    (predictions_test_max > 0.5).astype(\"int32\"),\n",
    "                    ax=ax_max,\n",
    "                    name=\"ROC fold {}\".format(i+1),\n",
    "                    alpha=0.3,\n",
    "            )\n",
    "\n",
    "            interp_tpr_avg = np.interp(mean_fpr_max, viz_max.fpr, viz_max.tpr)\n",
    "            interp_tpr_avg[0] = 0.0\n",
    "            tprs_max.append(interp_tpr_avg)\n",
    "            aucs_max.append(viz_max.roc_auc)\n",
    "\n",
    "\n",
    "        ############################################    \n",
    "        ############################################\n",
    "        ############################################\n",
    "        #Plot avg ROC curves (max aggregation)\n",
    "        mean_tpr_max = np.mean(tprs_max, axis=0)\n",
    "        mean_tpr_max[-1] = 1.0\n",
    "        mean_auc_max = auc(mean_fpr_max, mean_tpr_max)\n",
    "        std_auc_max = np.std(aucs_max)\n",
    "\n",
    "        ax_max.plot(\n",
    "            mean_fpr_max,\n",
    "            mean_tpr_max,\n",
    "            color=\"b\",\n",
    "            label=r\"Avg. ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_max, std_auc_max),\n",
    "            lw=2,\n",
    "            alpha=0.8,\n",
    "        )    \n",
    "\n",
    "        ax_max.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)    \n",
    "\n",
    "        ax_max.set(\n",
    "            xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic example MAX\",\n",
    "        )\n",
    "        ax_max.legend(loc=\"lower right\")\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "        ##################\n",
    "        ##################\n",
    "        #Plot avg ROC curves (mean aggregation)\n",
    "\n",
    "        mean_tpr_mean = np.mean(tprs_mean, axis=0)\n",
    "        mean_tpr_mean[-1] = 1.0\n",
    "        mean_auc_mean = auc(mean_fpr_mean, mean_tpr_mean)\n",
    "        std_auc_mean = np.std(aucs_mean)\n",
    "\n",
    "        ax_mean.plot(\n",
    "            mean_fpr_mean,\n",
    "            mean_tpr_mean,\n",
    "            color=\"b\",\n",
    "            label=r\"Max ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_mean, std_auc_mean),\n",
    "            lw=2,\n",
    "            alpha=0.8,\n",
    "        )    \n",
    "\n",
    "        ax_mean.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)    \n",
    "\n",
    "        ax_mean.set(\n",
    "            xlim=[-0.05, 1.05],\n",
    "            ylim=[-0.05, 1.05],\n",
    "            title=\"Receiver operating characteristic example MAX\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "        ax_mean.legend(loc=\"lower right\")\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "        ax_max.set_title(\"Receiver Operating Characteristic Curves, Max aggregation, MLP\")\n",
    "        ax_mean.set_title(\"Receiver Operating Characteristic Curves, Mean aggregation, MLP\")\n",
    "\n",
    "        ax_max.grid(linestyle=\"--\")\n",
    "        ax_mean.grid(linestyle=\"--\")\n",
    "\n",
    "        ################\n",
    "        ################\n",
    "        ################\n",
    "\n",
    "        ########################################\n",
    "        ########################################\n",
    "        print(\"Avg score training sets mean: {}\".format(np.mean(np.array(avg_score_train_mean))))\n",
    "        print(\"Avg score test sets mean: {}\".format(np.mean(np.array(avg_score_test_mean))))\n",
    "\n",
    "        print(\"Avg precision training sets mean: {}\".format(np.mean(np.array(avg_precision_train_mean))))\n",
    "        print(\"Avg precision test sets mean: {}\".format(np.mean(np.array(avg_precision_test_mean))))\n",
    "\n",
    "        print(\"Avg recall training sets mean: {}\".format(np.mean(np.array(avg_recall_train_mean))))\n",
    "        print(\"Avg recall test sets mean: {}\".format(np.mean(np.array(avg_recall_test_mean))))\n",
    "\n",
    "        print(\"Avg F1 training sets mean: {}\".format(np.mean(np.array(avg_f1_train_mean))))\n",
    "        print(\"Avg F1 test sets mean: {}\".format(np.mean(np.array(avg_f1_test_mean))))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Std score training sets mean: {}\".format(np.std(np.array(avg_score_train_mean))))\n",
    "        print(\"Std score test sets mean: {}\".format(np.std(np.array(avg_score_test_mean))))\n",
    "\n",
    "        print(\"Std precision training sets mean: {}\".format(np.std(np.array(avg_precision_train_mean))))\n",
    "        print(\"Std precision test sets mean: {}\".format(np.std(np.array(avg_precision_test_mean))))\n",
    "\n",
    "        print(\"Std recall training sets mean: {}\".format(np.std(np.array(avg_recall_train_mean))))\n",
    "        print(\"Std recall test sets mean: {}\".format(np.std(np.array(avg_recall_test_mean))))\n",
    "\n",
    "        print(\"Std F1 training sets mean: {}\".format(np.std(np.array(avg_f1_train_mean))))\n",
    "        print(\"Std F1 test sets mean: {}\".format(np.std(np.array(avg_f1_test_mean))))\n",
    "        #########################################\n",
    "        #########################################\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        ########################################\n",
    "        ########################################\n",
    "        print(\"\")\n",
    "        print(\"Avg score training sets max: {}\".format(np.mean(np.array(avg_score_train_max))))\n",
    "        print(\"Avg score test sets max: {}\".format(np.mean(np.array(avg_score_test_max))))\n",
    "\n",
    "        print(\"Avg precision training sets max: {}\".format(np.mean(np.array(avg_precision_train_max))))\n",
    "        print(\"Avg precision test sets max: {}\".format(np.mean(np.array(avg_precision_test_max))))\n",
    "\n",
    "        print(\"Avg recall training sets max: {}\".format(np.mean(np.array(avg_recall_train_max))))\n",
    "        print(\"Avg recall test sets max: {}\".format(np.mean(np.array(avg_recall_test_max))))\n",
    "\n",
    "        print(\"Avg F1 training sets max: {}\".format(np.mean(np.array(avg_f1_train_max))))\n",
    "        print(\"Avg F1 test sets max: {}\".format(np.mean(np.array(avg_f1_test_max))))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Std score training sets max: {}\".format(np.std(np.array(avg_score_train_max))))\n",
    "        print(\"Std score test sets max: {}\".format(np.std(np.array(avg_score_test_max))))\n",
    "\n",
    "        print(\"Std precision training sets max: {}\".format(np.std(np.array(avg_precision_train_max))))\n",
    "        print(\"Std precision test sets max: {}\".format(np.std(np.array(avg_precision_test_max))))\n",
    "\n",
    "        print(\"Std recall training sets max: {}\".format(np.std(np.array(avg_recall_train_max))))\n",
    "        print(\"Std recall test sets max: {}\".format(np.std(np.array(avg_recall_test_max))))\n",
    "\n",
    "        print(\"Std F1 training sets max: {}\".format(np.std(np.array(avg_f1_train_max))))\n",
    "        print(\"Std F1 test sets max: {}\".format(np.std(np.array(avg_f1_test_max))))\n",
    "\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")\n",
    "        print(\"####################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e608a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12def33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch-level cancer recognition.\n",
    "#100% and 0% cancer folders are selected.\n",
    "folder_tumor = ['TCGA-G7-7502-01A-01-BS1','TCGA-DW-7836-01A-01-TS1','TCGA-B8-A54K-01A-01-TSA','TCGA-B9-A8YH-01A-01-TS1'] \n",
    "folder_normal = ['TCGA-P4-A5E8-11A-01-TS1','TCGA-P4-A5ED-11A-01-TS1','TCGA-GL-A59R-11A-01-TS1'] \n",
    "\n",
    "test_tumor_path = r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data cancer\\thesis_data_jpg'\n",
    "test_normal_path = r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data cancer\\thesis_data_jpg'\n",
    "\n",
    "#declare lists to store predictions.\n",
    "prediction_tumor_mlp_list_mean = []\n",
    "prediction_normal_mlp_list_mean = []\n",
    "\n",
    "#Build test label list for patch-level classification problem.\n",
    "test_label_list = []\n",
    "\n",
    "image_path_test  = r'C:\\Users\\fabga\\OneDrive\\Desktop\\actual data cancer\\thesis_data_jpg'\n",
    "test_directories = ['TCGA-G7-7502-01A-01-BS1','TCGA-DW-7836-01A-01-TS1','TCGA-B8-A54K-01A-01-TSA','TCGA-B9-A8YH-01A-01-TS1',\n",
    "                    'TCGA-P4-A5E8-11A-01-TS1','TCGA-P4-A5ED-11A-01-TS1','TCGA-GL-A59R-11A-01-TS1']\n",
    "\n",
    "j=0\n",
    "for folder in test_directories:\n",
    "    #print(folder)\n",
    "    for element in os.listdir(image_path_test + '\\\\' + folder):\n",
    "        #print(len(dirnames))\n",
    "        j +=1\n",
    "        if folder in folder_normal:\n",
    "            label = 0\n",
    "        elif folder in folder_tumor:\n",
    "            label = 1\n",
    "        test_label_list.append(int(label))\n",
    "        \n",
    "\n",
    "#Initialize lists containing predictions over folders.         \n",
    "prediction_normal_mlp_list_mean = []\n",
    "prediction__mlp_list_mean = []\n",
    "j = 0\n",
    "\n",
    "#Initialize lists containing correctly classified tumor and healty tissues. \n",
    "correct_tumor  = []\n",
    "correct_healty = []\n",
    "\n",
    "#Make predictions on the sleected folders.        \n",
    "prediction_0_mlp_list_mean = []\n",
    "prediction_1_mlp_list_mean = []\n",
    "\n",
    "k = 0\n",
    "\n",
    "correct_tumor  = []\n",
    "correct_healty = []\n",
    "\n",
    "\n",
    "for folder in folder_1:\n",
    "    print(folder)\n",
    "    for element in os.listdir(test_1_path + '\\\\' + folder):\n",
    "        file_path = test_1_path + '\\\\' + folder + '\\\\' + element\n",
    "        \n",
    "        image = np.array(preprocess_input(img_to_array(load_img(file_path, target_size=(224,224)))))\n",
    "        image_batch = np.expand_dims(image, axis = 0) \n",
    "        image_features = model.predict(image_batch,batch_size=1)\n",
    "        image_features = image_features.reshape((image_features.shape[0], 7 * 7 * 2048))\n",
    "        predictions_mlp_mean = functional_mlp.predict(image_features)[0][0]\n",
    "        \n",
    "        #append results to list\n",
    "        prediction_1_mlp_list_mean.append((predictions_mlp_mean> 0.5).astype(\"int32\"))\n",
    "        #print(prediction_1_mlp_list_mean[k])\n",
    "        #print(test_label_list[k])        \n",
    "        #print(\"\")\n",
    "        \n",
    "        if  prediction_1_mlp_list_mean[k] == test_label_list[k]:\n",
    "            correct_tumor.append(file_path)\n",
    "            #print(prediction_1_mlp_list_mean[k])\n",
    "            #print(test_label_list[k])\n",
    "            #print(\"\")\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        k+=1\n",
    "        if k%100 == 0:\n",
    "            print(\"k = {}\".format(k))\n",
    "        \n",
    "print(k)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"###############################################\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "j = 0\n",
    "for folder in folder_0:\n",
    "    for element in os.listdir(test_0_path + '\\\\' + folder):\n",
    "        file_path = test_0_path + '\\\\' + folder + '\\\\' + element\n",
    "        image = np.array(preprocess_input(img_to_array(load_img(file_path, target_size=(224,224)))))\n",
    "        image_batch = np.expand_dims(image, axis = 0) \n",
    "        image_features = model.predict(image_batch,batch_size=1)\n",
    "        image_features = image_features.reshape((image_features.shape[0], 7 * 7 * 2048))\n",
    "        predictions_mlp_mean = functional_mlp.predict(image_features)[0][0]\n",
    "\n",
    "        #append results to list\n",
    "        prediction_0_mlp_list_mean.append((predictions_mlp_mean> 0.5).astype(\"int32\"))\n",
    "        #print(prediction_0_mlp_list_mean[j])\n",
    "        #print(test_label_list[j+k])        \n",
    "        #print(\"\")\n",
    "        \n",
    "        if  prediction_0_mlp_list_mean[j] == test_label_list[j+k+1]:\n",
    "            correct_healty.append(file_path)\n",
    "            #print(prediction_0_mlp_list_mean[j])\n",
    "            #print(test_label_list[j+k+1])\n",
    "            #print(\"\")\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        j+=1\n",
    "        if j%100 == 0:\n",
    "            print(\"j = {}\".format(j))\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b098eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix \n",
    "titles_options = [\n",
    "    (\"Patch classification, Holdout set Confusion Matrix\", None),\n",
    "]\n",
    "\n",
    "class_names = ['Normal','Tumor']\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        prediction_mlp_list_mean,\n",
    "        test_label_list,\n",
    "        #prediction_mlp_list_mean,\n",
    "        display_labels=class_names,\n",
    "        cmap=plt.cm.Reds,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute metrics for patch level classification.\n",
    "prediction_mlp_list_mean = prediction_tumor_mlp_list_mean + prediction_normal_mlp_list_mean\n",
    "accuracy_test_mean = accuracy_score(prediction_mlp_list_mean, test_label_list)\n",
    "f1_test_mean = f1_score(test_label_list, prediction_mlp_list_mean, average='binary')\n",
    "precision_test_mean = precision_score(test_label_list, prediction_mlp_list_mean, average='binary')\n",
    "recall_test_mean = recall_score(test_label_list, prediction_mlp_list_mean, average='binary')\n",
    "print(\"Accuracy mean mlp is: {}\".format(accuracy_test_mean))\n",
    "print(\"F1 mean mlp is: {}\".format(f1_test_mean))\n",
    "print(\"Precision mean mlp is: {}\".format(precision_test_mean))\n",
    "print(\"Recall mean mlp is: {}\".format(recall_test_mean))\n",
    "\n",
    "#Confusion matrix\n",
    "titles_options = [\n",
    "    (\"Patch classification, Holdout set Confusion Matrix\", None),\n",
    "]\n",
    "\n",
    "class_names = ['Healty','Tumor']\n",
    "for title, normalize in titles_options:\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        prediction_mlp_list_mean,\n",
    "        test_label_list,\n",
    "        display_labels=class_names,\n",
    "        cmap=plt.cm.Reds,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e426661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature activation maps.\n",
    "#Build a functional model of the MLP\n",
    "\n",
    "#functional model.\n",
    "input_ = tf.keras.layers.Input(shape=(7 * 7 * 2048,))\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(input_)            \n",
    "x = tf.keras.layers.Dense(224, activation='relu')(x) \n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)  \n",
    "predictions = x\n",
    "\n",
    "functional_mlp = Model(inputs=input_, outputs=predictions)\n",
    "functional_mlp.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#Fit the model on the appropriate data.\n",
    "functional_mlp.fit(X_mean, y_mean,epochs=500,verbose = 0)\n",
    "\n",
    "#build Resnet50 model.\n",
    "model = ResNet50(weights=\"imagenet\",input_shape=[224,224,3], include_top=False)\n",
    "\n",
    "#Connect resnet to the MLP.\n",
    "base_model = ResNet50(weights=\"imagenet\",input_shape=[224,224,3], include_top=False)\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = functional_mlp(x)\n",
    "predictions = x\n",
    "transfer_model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_img_array(img, size):\n",
    "    \n",
    "    #array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    img=np.resize(img,(size,size,3))#.astype(np.float32)\n",
    "    array = np.expand_dims(img, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    #print(grads)\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect images with and without tumors\n",
    "correct_tumor_paths = [correct_tumor[100],correct_tumor[20],correct_tumor[101]]\n",
    "correct_healty_paths = [correct_healty[7],correct_healty[8],correct_healty[9]]\n",
    "\n",
    "\n",
    "correct_tumor_vectors = []\n",
    "correct_tumor_labels=[]\n",
    "\n",
    "for path in correct_tumor_paths:\n",
    "    print(path)\n",
    "    img = img_to_array(load_img(path, target_size=(224, 224)))\n",
    "    image_converted = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    image_resized = tf.image.resize(image_converted, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    correct_tumor_vectors.append(image_resized)\n",
    "\n",
    "    \n",
    "correct_healty_vectors = []\n",
    "correct_halty_labels=[]\n",
    "for path in correct_healty_paths:\n",
    "    print(path)\n",
    "    img = img_to_array(load_img(path, target_size=(224, 224)))\n",
    "    image_converted = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    image_resized = tf.image.resize(image_converted, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    correct_healty_vectors.append(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c0dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=224\n",
    "IMG_HEIGHT=224\n",
    "IMG_WIDTH=224\n",
    "CHANNELS=3\n",
    "label = 0\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#img_size=128\n",
    "\n",
    "\n",
    "image_examples=[]\n",
    "labels_examples=[]\n",
    "cam_examples=[]\n",
    "\n",
    "\n",
    "for image in correct_tumor_vectors:\n",
    "    image_examples.append(image)\n",
    "    labels_examples.append(label)\n",
    "\n",
    "    # Prepare image\n",
    "\n",
    "    #image_converted = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # Resize image\n",
    "    #image_resized = tf.image.resize(image_converted, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array=tf.expand_dims(image,axis=0)\n",
    "    print(img_array.shape)\n",
    "    # Make model\n",
    "    model = transfer_model\n",
    "\n",
    "    # Remove last layer's softmax\n",
    "    model.layers[-1].activation = None\n",
    "\n",
    "    # Print what the top predicted class is\n",
    "\n",
    "    preds = (model.predict(img_array)>0.5).astype(\"int32\")[0][0]\n",
    "    #print(\"Label: \", label.numpy(), \"Predicted:\", np.argmax(preds[0]))#decode_predictions(preds, top=1)[0])\n",
    "    print(preds)\n",
    "    \n",
    "    last_conv_layer_name=\"conv5_block3_3_conv\"\n",
    "    # Generate class activation heatmap\n",
    "    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "    cam_examples.append(heatmap)\n",
    "\n",
    "    # Display last heatmap\n",
    "    plt.matshow(heatmap)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
